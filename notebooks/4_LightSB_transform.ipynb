{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d90f645",
   "metadata": {},
   "source": [
    "# Light Schrödinger Bridge Night to Day Domain Transform\n",
    "\n",
    "This notebooks trains the LightSB model using the AutoencoderKL latents we generated. It will perform a grid search over the n_potentials, is_diagonal and epsilon parameters to get the best performing model.\n",
    "\n",
    "Next, it performs a transformation on the night validation set. Finally, we decode these latents back into images using the encoder and save them separately to the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe3f99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f583efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# if colab, mount drive and get the git repo\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    print(os.getcwd())\n",
    "    drive.mount('/content/drive')\n",
    "    !git clone --recurse-submodules https://github.com/jsluijter02/LightSB_YOLO\n",
    "\n",
    "    # Append LightSB_YOLO path\n",
    "    sys.path.append(os.path.join(os.getcwd(), 'LightSB_YOLO'))\n",
    "\n",
    "    ## TODO: \n",
    "\n",
    "# otherwise local path append\n",
    "else:\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lightsb_yolo/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/lightsb_yolo/lib/python3.8/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from argparse import Namespace\n",
    "import copy\n",
    "import torch\n",
    "import diffusers\n",
    "from datetime import date\n",
    "\n",
    "from scripts.utils import dirs\n",
    "dirs.add_LIGHTSB_to_PATH()\n",
    "\n",
    "from scripts.models.autoencoderkl import AutoencoderKL_BDD\n",
    "from scripts.models.lightsb import LightSB_BDD\n",
    "from scripts.evals.FID import latent_FID_score, image_FID_score\n",
    "from scripts.utils import img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55922ea1",
   "metadata": {},
   "source": [
    "## Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43661621",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AutoencoderKL_BDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e360558",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d42f57",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jochem/Documents/GitHub/LightSB_YOLO/data/encodings/train_day.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m latent_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencodings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Only save Val_night file names -> others are redundant\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_day_latents, _  \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_latents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_day.npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_day_latents\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m train_night_latents, _ \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mload_latents(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(latent_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_night.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/GitHub/LightSB_YOLO/scripts/models/autoencoderkl.py:64\u001b[0m, in \u001b[0;36mAutoencoderKL_BDD.load_latents\u001b[0;34m(self, npz_filepath)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_latents\u001b[39m(\u001b[38;5;28mself\u001b[39m, npz_filepath):\n\u001b[0;32m---> 64\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpz_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatents\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilenames\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lightsb_yolo/lib/python3.8/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/jochem/Documents/GitHub/LightSB_YOLO/data/encodings/train_day.npz'"
     ]
    }
   ],
   "source": [
    "data_dir = dirs.get_data_dir()\n",
    "latent_dir = os.path.join(data_dir, \"encodings\")\n",
    "\n",
    "# Only save Val_night file names -> others are redundant\n",
    "train_day_latents, _  = encoder.load_latents(os.path.join(latent_dir, \"train_day.npz\"))\n",
    "print(train_day_latents.shape)\n",
    "\n",
    "train_night_latents, _ = encoder.load_latents(os.path.join(latent_dir, \"train_night.npz\"))\n",
    "print(\"train_night_latents shape: \", train_night_latents.shape)\n",
    "\n",
    "val_day_latents, _ = encoder.load_latents(os.path.join(latent_dir, \"val_day.npz\"))\n",
    "print(\"val_day_latents shape: \", val_day_latents.shape)\n",
    "\n",
    "val_night_latents, val_night_filenames = encoder.load_latents(os.path.join(latent_dir, \"val_night.npz\"))\n",
    "print(\"val_night_latents shape: \", val_night_latents.shape)\n",
    "print(\"val_night_filenames shape: \", val_night_filenames.shape)\n",
    "\n",
    "np_data = {\"train_day\": train_day_latents, \n",
    "           \"train_night\": train_night_latents, \n",
    "           \"val_day\": val_day_latents, \n",
    "           \"val_night\": val_night_latents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a78add",
   "metadata": {},
   "source": [
    "## Load Light Schrödinger Bridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6fad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_config = LightSB_BDD.standard_config()\n",
    "sb = LightSB_BDD(sb_config, np_data=np_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ebc52",
   "metadata": {},
   "source": [
    "## Train SB with a Grid Search + Val Set Evaluation\n",
    "To get a sense of parameter's effectiveness on the transformation, this step performs a grid search and saves the best parameters and state dictionary.\n",
    "\n",
    "Evaluation method: FID on latents.\n",
    "\n",
    "Due to computational and time constraints, it is not possible to transform ALL images six times and produce FID-scores, so a latent-based proxy is taken to determine final LightSB model parameters. FID metric on images will be taken of the final image set.\n",
    "\n",
    "This is different from actual FID metric, as this takes the metric on features learned from a CNN.\n",
    "\n",
    "Additionally, downstream, the mAP of the YOLOPX algorithm will determine this method's succesfullness as a preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe617d",
   "metadata": {},
   "source": [
    "### Grid Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdbf88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.01, 0.1, 0.5]\n",
    "n_potentials = [10, 20]\n",
    "is_diagonal = ... # Doesn't work on mac, but it \n",
    "max_steps = [1000, 10000, 50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef8b4d",
   "metadata": {},
   "source": [
    "### Grid Search Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "\n",
    "models = {}\n",
    "fid_scores = {}\n",
    "sample_indices = img.sample_indices(len(sb.X_test), how_many=5)\n",
    "\n",
    "best = {\n",
    "    \"name\": None,\n",
    "    \"fid\": None,\n",
    "    \"state_dict\": None,\n",
    "    \"epsilon\" : None,\n",
    "    \"max_steps\": None,\n",
    "    \"n_potentials\": None\n",
    "}\n",
    "\n",
    "date = date.today().strftime('%Y%m%d')\n",
    "runs_path = os.path.join(dirs.get_base_dir(), \"runs\", \"LightSB_GridSearch\", date)\n",
    "os.makedirs(runs_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398043c7",
   "metadata": {},
   "source": [
    "### Grid Search Loop + Internal Val Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c34347",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in epsilons:\n",
    "    for potential in n_potentials:\n",
    "        for steps in max_steps:\n",
    "            print(\"Started process for: \", steps, \" \", eps, \" \", potential)\n",
    "            # to reduce training time, reduce steps to 1000\n",
    "            args.MAX_STEPS = steps\n",
    "            args.EPSILON = eps\n",
    "            args.N_POTENTIALS = potential\n",
    "\n",
    "            print(\"Reloading model\")\n",
    "            sb.update_config(args=args)\n",
    "            sb.reload_model()\n",
    "            print(\"Reloaded model\")\n",
    "\n",
    "            print(\"Training model\")\n",
    "            sb.train()\n",
    "            print(\"Trained model\")\n",
    "\n",
    "            print(\"Transforming Validation Latents\")\n",
    "            transformed = sb.transform(sb.X_test)\n",
    "\n",
    "            fid = latent_FID_score(transformed, sb.Y_test)\n",
    "            print(\"FID-score on latents: \", fid)\n",
    "\n",
    "            state_dict = copy.deepcopy(sb.model.state_dict())\n",
    "\n",
    "            save = {\n",
    "                \"fid\": fid,\n",
    "                \"state_dict\": state_dict,\n",
    "                \"max_steps\": steps,\n",
    "                \"epsilon\": eps,\n",
    "                \"n_potentials\": potential\n",
    "            }\n",
    "\n",
    "            model_name = f'{fid}_{eps}_{potential}_{steps}.pt'\n",
    "\n",
    "            torch.save(save, os.path.join(runs_path, model_name))\n",
    "\n",
    "            # Save highest FID score's params\n",
    "            if fid > best[\"fid\"]:\n",
    "                best[\"fid\"] = fid\n",
    "                best[\"epsilon\"] = eps\n",
    "                best[\"max_steps\"] = steps\n",
    "                best[\"n_potentials\"] = potential\n",
    "                best[\"name\"] = model_name\n",
    "                best[\"state_dict\"] = state_dict\n",
    "\n",
    "            sample_imgs = diffusers.utils.pt_to_pil(encoder.decode_latents(transformed[sample_indices]))\n",
    "            img.plot_samples(sample_imgs, title=f'Transformed Val Set Images with Params: Steps: {steps}, Epsilon: {eps}, N_potentials: {potential}. FID: {fid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8200a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: None, FID: None\n"
     ]
    }
   ],
   "source": [
    "print(f'Best model: {best[\"name\"]}, FID: {best[\"fid\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57e37b",
   "metadata": {},
   "source": [
    "## Transform Val Images with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg = LightSB_BDD.standard_config()\n",
    "best_cfg.MODEL.EPSILON = 0.1 #best[\"epsilon\"]\n",
    "best_cfg.MODEL.N_POTENTIALS = 20 # best[\"n_potentials\"]\n",
    "best_cfg.MAX_STEPS = 10000 #best[\"max_steps\"]\n",
    "\n",
    "best_sb = LightSB_BDD(config=best_cfg, np_data=np_data)\n",
    "\n",
    "best_sb.train()\n",
    "#best_sb.load_state_dict(best[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_transformed = best_sb.transform(best_sb.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a0f4f",
   "metadata": {},
   "source": [
    "## Save Images to Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a51eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_imgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_night_filenames\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joche\\Development\\Github\\LightSB_YOLO\\scripts\\models\\autoencoderkl.py:93\u001b[0m, in \u001b[0;36mAutoencoderKL_BDD.save_imgs\u001b[1;34m(self, decoded_latents, filenames)\u001b[0m\n\u001b[0;32m     90\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m latent, filename \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(decoded_latents, filenames):\n\u001b[1;32m---> 93\u001b[0m     \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joche\\anaconda3\\envs\\yolopx_lightsb\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joche\\anaconda3\\envs\\yolopx_lightsb\\lib\\site-packages\\torchvision\\utils.py:147\u001b[0m, in \u001b[0;36msave_image\u001b[1;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    146\u001b[0m     _log_api_usage_once(save_image)\n\u001b[1;32m--> 147\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[39;00m\n\u001b[0;32m    149\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\joche\\anaconda3\\envs\\yolopx_lightsb\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joche\\anaconda3\\envs\\yolopx_lightsb\\lib\\site-packages\\torchvision\\utils.py:109\u001b[0m, in \u001b[0;36mmake_grid\u001b[1;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value)\u001b[0m\n\u001b[0;32m    107\u001b[0m xmaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(nrow, nmaps)\n\u001b[0;32m    108\u001b[0m ymaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mfloat\u001b[39m(nmaps) \u001b[38;5;241m/\u001b[39m xmaps))\n\u001b[1;32m--> 109\u001b[0m height, width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m padding), \u001b[38;5;28mint\u001b[39m(tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m+\u001b[39m padding)\n\u001b[0;32m    110\u001b[0m num_channels \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    111\u001b[0m grid \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mnew_full((num_channels, height \u001b[38;5;241m*\u001b[39m ymaps \u001b[38;5;241m+\u001b[39m padding, width \u001b[38;5;241m*\u001b[39m xmaps \u001b[38;5;241m+\u001b[39m padding), pad_value)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 2)"
     ]
    }
   ],
   "source": [
    "encoder.save_imgs(best_transformed, filenames=val_night_filenames, folder_name=\"LightSB_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf273959",
   "metadata": {},
   "source": [
    "## Get ACTUAL Val Night FID Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Day / Day FID\n",
    "## Night / Day FID\n",
    "## Day / Fake Day FID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10773f39",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolopx_lightsb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
