{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d90f645",
   "metadata": {},
   "source": [
    "# Light Schrödinger Bridge Night to Day Domain Transform\n",
    "\n",
    "This notebooks trains the LightSB model using the AutoencoderKL latents we generated. It will perform a grid search over the n_potentials, is_diagonal and epsilon parameters to get the best performing model.\n",
    "\n",
    "Next, it performs a transformation on the night validation set. Finally, we decode these latents back into images using the encoder and save them separately to the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe3f99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f583efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# if colab, mount drive and get the git repo\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    print(os.getcwd())\n",
    "    drive.mount('/content/drive')\n",
    "    !git clone --recurse-submodules https://github.com/jsluijter02/LightSB_YOLO\n",
    "\n",
    "    # Append LightSB_YOLO path\n",
    "    sys.path.append(os.path.join(os.getcwd(), 'LightSB_YOLO'))\n",
    "\n",
    "    ## TODO: \n",
    "\n",
    "# otherwise local path append\n",
    "else:\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e06e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lightsb_yolo/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/lightsb_yolo/lib/python3.8/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from argparse import Namespace\n",
    "import copy\n",
    "import torch\n",
    "import diffusers\n",
    "from datetime import date\n",
    "\n",
    "from scripts.utils import dirs\n",
    "dirs.add_LIGHTSB_to_PATH()\n",
    "\n",
    "from scripts.models.autoencoderkl import AutoencoderKL_BDD\n",
    "from scripts.models.lightsb import LightSB_BDD\n",
    "from scripts.evals.FID import latent_FID_score\n",
    "from scripts.utils import img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55922ea1",
   "metadata": {},
   "source": [
    "## Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43661621",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid `pretrained_model_name_or_path` provided. Please set it to a valid URL.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mAutoencoderKL_BDD\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/LightSB_YOLO/scripts/models/autoencoderkl.py:19\u001b[0m, in \u001b[0;36mAutoencoderKL_BDD.__init__\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# source: https://huggingface.co/docs/diffusers/api/models/autoencoderkl\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m get_device()\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoencoderKL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_single_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# no training, so set to eval mode\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lightsb_yolo/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lightsb_yolo/lib/python3.8/site-packages/diffusers/loaders/single_file_model.py:327\u001b[0m, in \u001b[0;36mFromOriginalModelMixin.from_single_file\u001b[0;34m(cls, pretrained_model_link_or_path_or_dict, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m pretrained_model_link_or_path_or_dict\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mload_single_file_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_link_or_path_or_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_mmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_mmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m DiffusersAutoQuantizer\u001b[38;5;241m.\u001b[39mfrom_config(quantization_config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lightsb_yolo/lib/python3.8/site-packages/diffusers/loaders/single_file_utils.py:454\u001b[0m, in \u001b[0;36mload_single_file_checkpoint\u001b[0;34m(pretrained_model_link_or_path, force_download, proxies, token, cache_dir, local_files_only, revision, disable_mmap, user_agent)\u001b[0m\n\u001b[1;32m    451\u001b[0m     pretrained_model_link_or_path \u001b[38;5;241m=\u001b[39m pretrained_model_link_or_path\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     repo_id, weights_name \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_repo_id_and_weights_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_link_or_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m     pretrained_model_link_or_path \u001b[38;5;241m=\u001b[39m _get_model_file(\n\u001b[1;32m    456\u001b[0m         repo_id,\n\u001b[1;32m    457\u001b[0m         weights_name\u001b[38;5;241m=\u001b[39mweights_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    465\u001b[0m     )\n\u001b[1;32m    467\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m load_state_dict(pretrained_model_link_or_path, disable_mmap\u001b[38;5;241m=\u001b[39mdisable_mmap)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lightsb_yolo/lib/python3.8/site-packages/diffusers/loaders/single_file_utils.py:404\u001b[0m, in \u001b[0;36m_extract_repo_id_and_weights_name\u001b[0;34m(pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_repo_id_and_weights_name\u001b[39m(pretrained_model_name_or_path):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_valid_url(pretrained_model_name_or_path):\n\u001b[0;32m--> 404\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `pretrained_model_name_or_path` provided. Please set it to a valid URL.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m([^/]+)/([^/]+)/(?:blob/main/)?(.+)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m     weights_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid `pretrained_model_name_or_path` provided. Please set it to a valid URL."
     ]
    }
   ],
   "source": [
    "encoder = AutoencoderKL_BDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e360558",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d42f57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m latent_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencodings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Only save Val_night file names -> others are redundant\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_day_latents, _  \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241m.\u001b[39mload_latents(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(latent_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_day.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_day_latents\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m train_night_latents, _ \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mload_latents(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(latent_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_night.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = dirs.get_data_dir()\n",
    "latent_dir = os.path.join(data_dir, \"encodings\")\n",
    "\n",
    "# Only save Val_night file names -> others are redundant\n",
    "train_day_latents, _  = encoder.load_latents(os.path.join(latent_dir, \"train_day.npz\"))\n",
    "print(train_day_latents.shape)\n",
    "\n",
    "train_night_latents, _ = encoder.load_latents(os.path.join(latent_dir, \"train_night.npz\"))\n",
    "print(\"train_night_latents shape: \", train_night_latents.shape)\n",
    "\n",
    "val_day_latents, _ = encoder.load_latents(os.path.join(latent_dir, \"val_day.npz\"))\n",
    "print(\"val_day_latents shape: \", val_day_latents.shape)\n",
    "\n",
    "val_night_latents, val_night_filenames = encoder.load_latents(os.path.join(latent_dir, \"val_night.npz\"))\n",
    "print(\"val_night_latents shape: \", val_night_latents.shape)\n",
    "print(\"val_night_filenames shape: \", val_night_filenames.shape)\n",
    "\n",
    "np_data = {\"train_day\": train_day_latents, \n",
    "           \"train_night\": train_night_latents, \n",
    "           \"val_day\": val_day_latents, \n",
    "           \"val_night\": val_night_latents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a78add",
   "metadata": {},
   "source": [
    "## Load Light Schrödinger Bridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b6fad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_config = LightSB_BDD.standard_config()\n",
    "sb = LightSB_BDD(sb_config, np_data=np_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ebc52",
   "metadata": {},
   "source": [
    "## Train SB with a Grid Search + Val Set Evaluation\n",
    "To get a sense of parameter's effectiveness on the transformation, this step performs a grid search and saves the best parameters and state dictionary.\n",
    "\n",
    "Evaluation method: FID on latents.\n",
    "\n",
    "Due to computational and time constraints, it is not possible to transform ALL images six times and produce FID-scores, so a latent-based proxy is taken to determine final LightSB model parameters. FID metric on images will be taken of the final image set.\n",
    "\n",
    "This is different from actual FID metric, as this takes the metric on features learned from a CNN.\n",
    "\n",
    "Additionally, downstream, the mAP of the YOLOPX algorithm will determine this method's succesfullness as a preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe617d",
   "metadata": {},
   "source": [
    "### Grid Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bdbf88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.01, 0.1, 0.5]\n",
    "n_potentials = [10, 20]\n",
    "is_diagonal = ... # Doesn't work on mac, but it \n",
    "max_steps = [1000, 10000, 50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef8b4d",
   "metadata": {},
   "source": [
    "### Grid Search Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798a2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "\n",
    "models = {}\n",
    "fid_scores = {}\n",
    "sample_indices = img.sample_indices(len(sb.X_test), how_many=5)\n",
    "\n",
    "best = {\n",
    "    \"name\": None,\n",
    "    \"fid\": None,\n",
    "    \"state_dict\": None,\n",
    "    \"epsilon\" : None,\n",
    "    \"max_steps\": None,\n",
    "    \"n_potentials\": None\n",
    "}\n",
    "\n",
    "date = date.today().strftime('%Y%m%d')\n",
    "runs_path = os.path.join(dirs.get_base_dir(), \"runs\", \"LightSB_GridSearch\", date)\n",
    "os.makedirs(runs_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398043c7",
   "metadata": {},
   "source": [
    "### Grid Search Loop + Internal Val Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c34347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for eps in epsilons:\n",
    "#     for potential in n_potentials:\n",
    "#         for steps in max_steps:\n",
    "#             print(\"Started process for: \", steps, \" \", eps, \" \", potential)\n",
    "#             # to reduce training time, reduce steps to 1000\n",
    "#             args.MAX_STEPS = steps\n",
    "#             args.EPSILON = eps\n",
    "#             args.N_POTENTIALS = potential\n",
    "\n",
    "#             print(\"Reloading model\")\n",
    "#             sb.update_config(args=args)\n",
    "#             sb.reload_model()\n",
    "#             print(\"Reloaded model\")\n",
    "\n",
    "#             print(\"Training model\")\n",
    "#             sb.train()\n",
    "#             print(\"Trained model\")\n",
    "\n",
    "#             print(\"Transforming Validation Latents\")\n",
    "#             transformed = sb.transform(sb.X_test)\n",
    "\n",
    "#             fid = latent_FID_score(transformed, sb.Y_test)\n",
    "#             print(\"FID-score on latents: \", fid)\n",
    "\n",
    "#             state_dict = copy.deepcopy(sb.model.state_dict())\n",
    "\n",
    "#             save = {\n",
    "#                 \"fid\": fid,\n",
    "#                 \"state_dict\": state_dict,\n",
    "#                 \"max_steps\": steps,\n",
    "#                 \"epsilon\": eps,\n",
    "#                 \"n_potentials\": potential\n",
    "#             }\n",
    "\n",
    "#             model_name = f'{fid}_{eps}_{potential}_{steps}.pt'\n",
    "\n",
    "#             torch.save(save, os.path.join(runs_path, model_name))\n",
    "\n",
    "#             # Save highest FID score's params\n",
    "#             if fid > best[\"fid\"]:\n",
    "#                 best[\"fid\"] = fid\n",
    "#                 best[\"epsilon\"] = eps\n",
    "#                 best[\"max_steps\"] = steps\n",
    "#                 best[\"n_potentials\"] = potential\n",
    "#                 best[\"name\"] = model_name\n",
    "#                 best[\"state_dict\"] = state_dict\n",
    "\n",
    "#             sample_imgs = diffusers.utils.pt_to_pil(encoder.decode_latents(transformed[sample_indices]))\n",
    "#             img.plot_samples(sample_imgs, title=f'Transformed Val Set Images with Params: Steps: {steps}, Epsilon: {eps}, N_potentials: {potential}. FID: {fid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da8200a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: None, FID: None\n"
     ]
    }
   ],
   "source": [
    "print(f'Best model: {best[\"name\"]}, FID: {best[\"fid\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57e37b",
   "metadata": {},
   "source": [
    "## Transform Val Images with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c0a3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg = LightSB_BDD.standard_config()\n",
    "best_cfg.MODEL.EPSILON = 0.1 #best[\"epsilon\"]\n",
    "best_cfg.MODEL.N_POTENTIALS = 10 # best[\"n_potentials\"]\n",
    "best_cfg.MAX_STEPS = 10000 # best[\"max_steps\"]\n",
    "\n",
    "best_sb = sb # LightSB_BDD(config=best_cfg, np_data=np_data)\n",
    "\n",
    "# best_sb.load_state_dict(best[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88dd6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_transformed = best_sb.transform(best_sb.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a0f4f",
   "metadata": {},
   "source": [
    "## Save Images to Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b48a51eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_imgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_night_filenames\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joche\\Development\\Github\\LightSB_YOLO\\scripts\\models\\autoencoderkl.py:93\u001b[0m, in \u001b[0;36mAutoencoderKL_BDD.save_imgs\u001b[1;34m(self, decoded_latents, filenames)\u001b[0m\n\u001b[0;32m     90\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m latent, filename \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(decoded_latents, filenames):\n\u001b[1;32m---> 93\u001b[0m     \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joche\\anaconda3\\envs\\yolopx_lightsb\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joche\\anaconda3\\envs\\yolopx_lightsb\\lib\\site-packages\\torchvision\\utils.py:147\u001b[0m, in \u001b[0;36msave_image\u001b[1;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    146\u001b[0m     _log_api_usage_once(save_image)\n\u001b[1;32m--> 147\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[39;00m\n\u001b[0;32m    149\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\joche\\anaconda3\\envs\\yolopx_lightsb\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joche\\anaconda3\\envs\\yolopx_lightsb\\lib\\site-packages\\torchvision\\utils.py:109\u001b[0m, in \u001b[0;36mmake_grid\u001b[1;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value)\u001b[0m\n\u001b[0;32m    107\u001b[0m xmaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(nrow, nmaps)\n\u001b[0;32m    108\u001b[0m ymaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mfloat\u001b[39m(nmaps) \u001b[38;5;241m/\u001b[39m xmaps))\n\u001b[1;32m--> 109\u001b[0m height, width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m padding), \u001b[38;5;28mint\u001b[39m(tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m+\u001b[39m padding)\n\u001b[0;32m    110\u001b[0m num_channels \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    111\u001b[0m grid \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mnew_full((num_channels, height \u001b[38;5;241m*\u001b[39m ymaps \u001b[38;5;241m+\u001b[39m padding, width \u001b[38;5;241m*\u001b[39m xmaps \u001b[38;5;241m+\u001b[39m padding), pad_value)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 2)"
     ]
    }
   ],
   "source": [
    "encoder.save_imgs(best_transformed, filenames=val_night_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf273959",
   "metadata": {},
   "source": [
    "## Get ACTUAL Val Night FID Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10773f39",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightsb_yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
